{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your first model\n",
    "\n",
    "This is the second of our [beginner tutorial series](https://github.com/awslabs/djl/tree/master/jupyter/tutorial) that will take you through creating, training, and running inference on a neural network. In this tutorial, you will learn how to train an image classification model that can recognize handwritten digits.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "This tutorial requires the installation of the Java Jupyter Kernel. To install the kernel, see the [Jupyter README](https://github.com/awslabs/djl/blob/master/jupyter/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add the snapshot repository to get the DJL snapshot artifacts\n",
    "%mavenRepo snapshots https://oss.sonatype.org/content/repositories/snapshots/\n",
    "\n",
    "// Add the maven dependencies\n",
    "%maven ai.djl:api:0.3.0-SNAPSHOT\n",
    "%maven ai.djl:basicdataset:0.3.0-SNAPSHOT\n",
    "%maven ai.djl:model-zoo:0.3.0-SNAPSHOT\n",
    "%maven ai.djl:repository:0.3.0-SNAPSHOT\n",
    "%maven ai.djl.mxnet:mxnet-engine:0.3.0-SNAPSHOT\n",
    "%maven org.slf4j:slf4j-api:1.7.26\n",
    "%maven org.slf4j:slf4j-simple:1.7.26\n",
    "%maven net.java.dev.jna:jna:5.3.0\n",
    "        \n",
    "// See https://github.com/awslabs/djl/blob/master/mxnet/mxnet-engine/README.md\n",
    "// for more MXNet library selection options\n",
    "%maven ai.djl.mxnet:mxnet-native-auto:1.6.0-c-SNAPSHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.file.*;\n",
    "\n",
    "import ai.djl.*;\n",
    "import ai.djl.basicdataset.*;\n",
    "import ai.djl.ndarray.types.*;\n",
    "import ai.djl.training.*;\n",
    "import ai.djl.training.dataset.*;\n",
    "import ai.djl.training.initializer.*;\n",
    "import ai.djl.training.loss.*;\n",
    "import ai.djl.training.evaluator.*;\n",
    "import ai.djl.training.optimizer.*;\n",
    "import ai.djl.training.optimizer.learningrate.*;\n",
    "import ai.djl.training.util.*;\n",
    "import ai.djl.basicmodelzoo.cv.classification.*;\n",
    "import ai.djl.basicmodelzoo.basic.*;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare MNIST dataset for training\n",
    "\n",
    "When training a deep learning network, it is important to first understand the dataset.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "A [Dataset](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/dataset/Dataset.html) is a collection of sample input/output pairs for the function represented by your neural network. Each single input/output is represented by a [Record](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/dataset/Record.html). Each record could have multiple arrays of inputs or outputs such as an image question and answer dataset where the input is both an image and a question about the image while the output is the answer to the question.\n",
    "\n",
    "Because data learning is highly parallelizable, training is often done not with a single record at a time but a [Batch](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/dataset/Batch.html) of records at a time. This can lead to significant performance gains, especially when working with images.\n",
    "\n",
    "### MNIST\n",
    "\n",
    "The dataset we will be using is [MNIST](https://en.wikipedia.org/wiki/MNIST_database), a database of handwritten digits. Each image contains a black and white digit from 0-9 in a 28x28 image. It is commonly used when getting started with deep learning because it is small and fast to train.\n",
    "\n",
    "![Mnist Image](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "Once you understand your dataset, you should create an implementation of the [Dataset class](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/dataset/Dataset.html). In this case, we provide the MNIST dataset built-in to make it easy for you to use it.\n",
    "\n",
    "## Sampler\n",
    "\n",
    "Then, we must decide the parameters for loading data from the dataset. The only parameter we need for MNIST is the choice of [Sampler](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/dataset/Sampler.html). The sampler decides which and how many element from datasets are part of each batch when iterating through it. We will have it randomly shuffle the elements for the batch and use a batchSize of 32. The batchSize is usually the largest power of 2 that fits within memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int batchSize = 32;\n",
    "Mnist mnist = Mnist.builder().setSampling(batchSize, true).build();\n",
    "mnist.prepare(new ProgressBar());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create your Model\n",
    "\n",
    "A [Model](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/Model.html) contains a neural network [Block](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/nn/Block.html) along with additional artifacts used for the training process. It possesses additional information about the inputs, outputs, shapes, and data types you will use. Generally, you will use Model once you have fully completed your Block.\n",
    "\n",
    "In this tutoral, we will use the built-in Multilayer Perceptron Block from the Model Zoo. To learn more, see the previous tutorial: [Create Your First Network](create_your_first_network.ipynb).\n",
    "\n",
    "Because images in the MNIST dataset are 28x28 grayscale images, we will create an MLP block with 28 x 28 input. The output will be 10 because there are 10 possible classes (0 to 9) each image could be. For the hidden layers, we have chosen `new int[] {128, 64}` by experimenting with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model model = Model.newInstance();\n",
    "model.setBlock(new Mlp(28 * 28, 10, new int[] {128, 64}));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create a Trainer\n",
    "\n",
    "Now, you can create a [`Trainer`](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/Trainer.html) to train your model. The trainer is the main class to orchestrate the training process. Usually, they will be opened using a try-with-resources and closed after training is over.\n",
    "\n",
    "The trainer takes an existing model and attempts to optimize the parameters inside the model's Block to best match the dataset. Most optimization is based upon [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD).\n",
    "\n",
    "## Step 3.1: Setup your training configurations\n",
    "\n",
    "Before you create your trainer, we we will need a [training configuration](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/DefaultTrainingConfig.html) that describes how to train your model.\n",
    "\n",
    "The following are a few common items you may need to configure your training:\n",
    "* **REQUIRED** [`Loss`](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/loss/Loss.html) function: A loss function is used to measure how well our model matches the dataset. Because the lower value of the function is better, it's called the \"loss\" function. The Loss is the only required argument to the model\n",
    "* [`Evaluator`](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/evaluator/Evaluator.html) function: An evaluator function is also used to measure how well our model matches the dataset. Unlike the loss, they are only there for people to look at and are not used for optimizing the model. Since many losses are not as intuitive, adding other evaluators such as Accuracy can help to understand how your model is doing. If you know of any useful evaluators, we recommend adding them.\n",
    "* batch size: To take the advantage of the natural parallelism, you usually train models with batches of input data items rather than a single item at a time. This should match the batch size provided in the model\",\n",
    "* [`Device`](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/Device.html): The device is what hardware should be used to train your model on. Typically, this is either CPU or GPU. DJL can automatically detect whether a GPU is available. If GPUs are available, it will run on a single GPU by default. If you need to train with multiple GPUs, you need to set devices as : `config.setDevices(Devices.getDevices(maxNumberOfGPUs))`.\n",
    "* [`Initializer`](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/initializer/Initializer.html): An `Initializer` is used to set the initial values of the model's parameters before training. This can usually be left as the default initializer.\n",
    "* [`Optimizer`](https://javadoc.djl.ai/api/0.2.1/index.html?ai/djl/training/optimizer/Optimizer.html): The optimizer is the code that updates the model parameters to minimize the loss function. There are a variety of optimizers, most of which offer improvements upon the basic SGD. When just starting, you can use the default optimizer. Later on, Customizing the optimizer can result in faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig config = new DefaultTrainingConfig(Loss.softmaxCrossEntropyLoss())\n",
    "    //softmaxCrossEntropyLoss is a standard loss for classification problems\n",
    "        .addEvaluator(new Accuracy()) // Use accuracy so we humans can understand how accurate the model is\n",
    "        .setBatchSize(batchSize); // Use the same batch size as the dataset sampler\n",
    "\n",
    "// Now that we have our training configuration, we should create a new trainer for our model\n",
    "Trainer trainer = model.newTrainer(config);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Initialize Training\n",
    "\n",
    "Before training your model, you have to initialize all of the parameters with default values. You can use the trainer for this initialization by passing in the input shape.\n",
    "\n",
    "* The first axis of the input shape is the batch size. This won't impact the parameter initialization, so you can use 1 here.\n",
    "* The second axis of the input shape of the MLP - the number of pixels in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.initialize(new Shape(1, 28 * 28));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train your model\n",
    "\n",
    "Now, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30mProgressBar progressBar = new ProgressBar(\"Training\", (int) \u001b[0m\u001b[1m\u001b[30m\u001b[41mmnist.getIteration\u001b[0m\u001b[1m\u001b[30m());\u001b[0m",
      "\u001b[1m\u001b[31mcannot find symbol\u001b[0m",
      "\u001b[1m\u001b[31m  symbol:   method getIteration()\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "// Setup the progress bar to display the training progress\n",
    "ProgressBar progressBar = new ProgressBar(\"Training\", (int) mnist.getIteration());\n",
    "\n",
    "// Deep learning is typically trained in epochs where each epoch trains the model on each item in the dataset once.\n",
    "int epoch = 2;\n",
    "\n",
    "float trainingAccuracy = 0f;\n",
    "for (int i = 0; i < epoch; ++i) {\n",
    "    int index = 0;\n",
    "    \n",
    "    // We iterate through the dataset once during this epoch\n",
    "    for (Batch batch : trainer.iterateDataset(mnist)) {\n",
    "        \n",
    "        // During trainBatch, we update the loss and evaluators with the results for the training batch.\n",
    "        trainer.trainBatch(batch);\n",
    "        \n",
    "        // Now, we update the model parameters based on the results of the latest trainBatch\n",
    "        trainer.step();\n",
    "        \n",
    "        // We must make sure to close the batch to ensure all the memory associated with the batch is cleared quickly.\n",
    "        // If the memory isn't closed after each batch, you will very quickly run out of memory on your GPU\n",
    "        batch.close();\n",
    "\n",
    "        trainingAccuracy = accuracy.getValue();\n",
    "        progressBar.update(index++, String.format(\"Epoch: %d, Accuracy: %.3f\", i, trainingAccuracy));\n",
    "    }\n",
    "    // reset training and validation evaluators at the end of the epoch\n",
    "    trainer.resetEvaluators();\n",
    "}\n",
    "\n",
    "// If you did not use a try-with-resources with your trainer, close it once training is over\n",
    "trainer.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Save your model\n",
    "\n",
    "Once your model is trained, you should save it so that it can be reloaded later. You can also add metadata to it such as training accuracy, number of epochs trained, etc that can be used when loading the model or when examining it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path modelDir = Paths.get(\"build/mlp\");\n",
    "Files.createDirectories(modelDir);\n",
    "\n",
    "model.setProperty(\"Epoch\", String.valueOf(epoch));\n",
    "model.setProperty(\"Accuracy\", String.valueOf(trainingAccuracy));\n",
    "\n",
    "model.save(modelDir, \"mlp\");\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Now, you've successfully trained a model that can recognize handwritten digits. You'll learn how to apply this model in the next chapter: [Run image classification with your model](image_classification_with_your_model.ipynb).\n",
    "\n",
    "You can find the complete source code for this tutorial in the [examples project](https://github.com/awslabs/djl/blob/master/examples/src/main/java/ai/djl/examples/training/TrainMnist.java)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "13.0.2+8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning on CIFAR-10 Dataset\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, you learn how to train an image classification model using [Transfer Learning](https://en.wikipedia.org/wiki/Transfer_learning). Transfer learning is a popular machine learning technique that uses a model trained on one problem and applies it to a second related problem. Compared to training from scratch or designing a model for your specific problem, transfer learning can leverage the features already learned on a similar problem and produce a more robust model in a much shorter time.\n",
    "\n",
    "Train your model with the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset which consists of 60,000 32x32 color images in 10 classes. As for the pre-trained model, use the ResNet50v1[1] model. It's a 50 layer deep model already trained on [ImageNet](http://www.image-net.org/), a much larger dataset consisting of over 1.2 million images in 1000 classes. Modify it to classify 10 classes from the CIFAR-10 dataset.\n",
    "\n",
    "![The CIFAR-10 Dataset](https://djl-ai.s3.amazonaws.com/resources/images/cifar-10.png)\n",
    "<center>the CIFAR10 dataset</center>\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "This tutorial assumes you have the following knowledge. Follow the READMEs and tutorials if you are not familiar with:\n",
    "1. How to setup and run [Java Kernel in Jupyter Notebook](https://github.com/awslabs/djl/blob/master/jupyter/README.md)\n",
    "2. Basic components of Deep Java Library, and how to [train your first model](https://github.com/awslabs/djl/blob/master/jupyter/tutorial/train_your_first_model.ipynb).\n",
    "\n",
    "\n",
    "## Getting started\n",
    "Load the Deep Java Libarary and its dependencies from Maven:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mavenRepo snapshots https://oss.sonatype.org/content/repositories/snapshots/\n",
    "\n",
    "%maven ai.djl:api:0.3.0-SNAPSHOT\n",
    "%maven ai.djl:basicdataset:0.3.0-SNAPSHOT\n",
    "%maven ai.djl:model-zoo:0.3.0-SNAPSHOT\n",
    "%maven ai.djl:repository:0.3.0-SNAPSHOT\n",
    "%maven ai.djl.mxnet:mxnet-engine:0.3.0-SNAPSHOT\n",
    "%maven ai.djl.mxnet:mxnet-model-zoo:0.3.0-SNAPSHOT\n",
    "%maven org.slf4j:slf4j-api:1.7.26\n",
    "%maven org.slf4j:slf4j-simple:1.7.26\n",
    "%maven net.java.dev.jna:jna:5.3.0\n",
    "        \n",
    "// See https://github.com/awslabs/djl/blob/master/mxnet/mxnet-engine/README.md\n",
    "// for more MXNet library selection options\n",
    "%maven ai.djl.mxnet:mxnet-native-auto:1.6.0-c-SNAPSHOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.*;\n",
    "import ai.djl.basicdataset.*;\n",
    "import ai.djl.modality.cv.transform.*;\n",
    "import ai.djl.mxnet.zoo.*;\n",
    "import ai.djl.ndarray.*;\n",
    "import ai.djl.ndarray.types.*;\n",
    "import ai.djl.nn.*;\n",
    "import ai.djl.nn.core.*;\n",
    "import ai.djl.training.*;\n",
    "import ai.djl.training.dataset.*;\n",
    "import ai.djl.training.initializer.*;\n",
    "import ai.djl.training.loss.*;\n",
    "import ai.djl.training.evaluator.*;\n",
    "import ai.djl.training.optimizer.*;\n",
    "import ai.djl.training.optimizer.learningrate.*;\n",
    "import ai.djl.training.util.*;\n",
    "import ai.djl.translate.*;\n",
    "import ai.djl.basicmodelzoo.cv.classification.*;\n",
    "import java.nio.file.*;\n",
    "import java.util.*;\n",
    "import java.util.concurrent.*;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct your model\n",
    "\n",
    "Load the pre-trained ResNet50V1 model. You can find it in the [Model Zoo](https://github.com/awslabs/djl/blob/master/docs/model-zoo.md). First construct the `criteria` to specify which ResNet model to load, then call `loadModel` to get a ResNet50V1 model with pre-trained weights. Note this model was trained on ImageNet with 1000 classes; the last layer is a Linear layer with 1000 output channels. Because you are repurposing it on CIFAR10 with 10 classes, you need to remove the last layer and add a new Linear layer with 10 output channels. After you are done modifying the block, set it back to model using `setBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load model and change last layer\n",
    "Map<String, String> criteria = new ConcurrentHashMap<>();\n",
    "criteria.put(\"layers\", \"50\");\n",
    "criteria.put(\"flavor\", \"v1\");\n",
    "Model model = MxModelZoo.RESNET.loadModel(criteria, new ProgressBar());\n",
    "SequentialBlock newBlock = new SequentialBlock();\n",
    "SymbolBlock block = (SymbolBlock) model.getBlock();\n",
    "block.removeLastBlock();\n",
    "newBlock.add(block);\n",
    "newBlock.add(x -> new NDList(x.singletonOrThrow().squeeze()));\n",
    "newBlock.add(Linear.builder().setOutChannels(10).build());\n",
    "newBlock.add(Blocks.batchFlattenBlock());\n",
    "model.setBlock(newBlock);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "After you have the model, the next step is to prepare the dataset for training. You can construct a CIFAR10 builder with your own specifications. You have the options to get the train or test dataset, specify desired batch size, specify whether to shuffle your data during training, and most importantly, specify the pre-process pipeline. \n",
    "\n",
    "A pipeline consists of a series of transformations to apply on the input data before feeding it to the model. \n",
    "\n",
    "For example, `ToTensor` can be used to transform colored image NDArrays with shape (32, 32, 3) and values from 0 to 256 to NDArrays with shape (3, 32, 32) and values from 0 to 1. This operation is transposing image data from channels last to channels first format, which is more suitable for GPU computation. \n",
    "\n",
    "The `Normalize` transformation can normalize input data according to their mean and standard deviation values. This will make different features have similar range and help our model perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int batchSize = 32;\n",
    "int maxIteration = Integer.MAX_VALUE; // change this to a small value for a dry run\n",
    "// int maxIteration = 10; // run 10 teration for a dry run\n",
    "Pipeline pipeline = new Pipeline(\n",
    "    new ToTensor(),\n",
    "    new Normalize(new float[] {0.4914f, 0.4822f, 0.4465f}, new float[] {0.2023f, 0.1994f, 0.2010f}));\n",
    "Cifar10 trainDataset = \n",
    "    Cifar10.builder(model.getNDManager())\n",
    "    .setSampling(batchSize, true)\n",
    "    .optUsage(Dataset.Usage.TRAIN)\n",
    "    .optMaxIteration(maxIteration)\n",
    "    .optPipeline(pipeline)\n",
    "    .build();\n",
    "trainDataset.prepare(new ProgressBar());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training configuration\n",
    "\n",
    "You are leveraging a pre-trained model, so you can expect the model to converge quickly. You will only train only ten epochs. As the model converges, you need to reduce the learning rate to get better results. You can use a `LearningRateTracker` to reduce the learning rate by 0.1 after two, five, and eight epochs. \n",
    "\n",
    "Deep Java Library supports training on multiple GPUs. You can use `setDevices` and pass an array of devices you want the model to be trained on. For example, `new Device[]{Device.gpu(0), Device.gpu(1)}` for training on GPU0 and GPU1. You can also call `Device.getDevices(4)` and pass the number of GPUs you want to train. It will start with GPU0, and use CPU if no GPU is available. To learn more about multi-GPU training, read our multi-GPU [documentation](https://github.com/awslabs/djl/tree/master/examples/docs).\n",
    "\n",
    "To complete the training configuration set up, use the `Adam` optimizer, `SoftmaxCrossEntropyLoss`, and `Accuracy` for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int[] epochs = new int[] {2, 5, 8};\n",
    "int[] steps = Arrays.stream(epochs).map(k -> k * 60000 / batchSize).toArray();\n",
    "Initializer initializer = new XavierInitializer();\n",
    "MultiFactorTracker learningRateTracker = \n",
    "    LearningRateTracker.multiFactorTracker()\n",
    "    .setSteps(steps)\n",
    "    .optBaseLearningRate(1e-3f)\n",
    "    .optFactor((float) Math.sqrt(.1f))\n",
    "    .optWarmUpBeginLearningRate(1e-4f)\n",
    "    .optWarmUpSteps(200)\n",
    "    .build();\n",
    "Optimizer optimizer = Optimizer.adam()\n",
    "    .setRescaleGrad(1.0f / batchSize)\n",
    "    .optLearningRateTracker(learningRateTracker)\n",
    "    .optWeightDecays(0.001f)\n",
    "    .optClipGrad(5f)\n",
    "    .build();\n",
    "Loss loss = Loss.softmaxCrossEntropyLoss();\n",
    "Accuracy accuracy = new Accuracy();\n",
    "\n",
    "TrainingConfig config =  new DefaultTrainingConfig(initializer, loss)\n",
    "    .setOptimizer(optimizer)\n",
    "    .setBatchSize(batchSize)\n",
    "    .setDevices(Device.getDevices(1))\n",
    "    .addEvaluator(accuracy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "Now you can start training. This procedure is similar to the one in [Train Your First Model](https://github.com/awslabs/djl/blob/master/jupyter/tutorial/train_your_first_model.ipynb). Training requires the following steps:\n",
    "1. Initialize a new trainer using the training config you just set up\n",
    "2. Initialize the weights in trainer\n",
    "3. Using a `for` loop to iterate through the whole dataset 10 times (epochs), resetting the evaluators at the end of each epoch\n",
    "4. During each epoch, using a `for` loop to iterate through the dataset in batches and train batch by batch while printing the training accuracy on the progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProgressBar progressBar = new ProgressBar(\"Training\", (int)trainDataset.getIteration());\n",
    "int epoch = 10;\n",
    "Trainer trainer = model.newTrainer(config);\n",
    "Shape inputShape = new Shape(1, 3, 32, 32);\n",
    "trainer.initialize(inputShape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float trainingAccuracy = 0f;\n",
    "for (int i = 0; i < epoch; ++i) {\n",
    "    int index = 0;\n",
    "    for (Batch batch : trainer.iterateDataset(trainDataset)) {\n",
    "        trainer.trainBatch(batch);\n",
    "        trainer.step();\n",
    "        batch.close();\n",
    "\n",
    "        trainingAccuracy = accuracy.getValue();\n",
    "        progressBar.update(index++, String.format(\"Epoch: %d, Accuracy: %.3f\", i, trainingAccuracy));\n",
    "    }\n",
    "    // reset training and validation evaluators at end of epoch\n",
    "    trainer.resetEvaluators();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "Finally, you can save your model after training is done and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path modelDir = Paths.get(\"build/resnet\");\n",
    "Files.createDirectories(modelDir);\n",
    "\n",
    "model.setProperty(\"Epoch\", String.valueOf(epoch));\n",
    "model.setProperty(\"Accuracy\", String.valueOf(trainingAccuracy));\n",
    "model.save(modelDir, \"resnet\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the `fit` method\n",
    "Instead of writing the two `for` loops, you can use the `fit` method in [TrainingUtils](TrainingUtils.java), which will handle everything automatically. Just pass your `trainer`, number of epochs to train, training dataset, validation dataset (if any), model output path, and model name. It will save your model checkpoint at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load TrainingUtils.java\n",
    "\n",
    "ProgressBar progressBar = new ProgressBar(\"Training\", (int)trainDataset.getIteration());\n",
    "int epoch = 10;\n",
    "Trainer trainer = model.newTrainer(config);\n",
    "Shape inputShape = new Shape(1, 3, 32, 32);\n",
    "trainer.initialize(inputShape);\n",
    "\n",
    "trainer.addTrainingListeners(TrainingUtils.getTrainingListener(progressBar, null));\n",
    "\n",
    "TrainingUtils.fit(trainer, epoch, trainDataset, null, \"build/resnet\", \"resnetv1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "1. Try inference using the model you just trained. You can find an airplane image in [test resources](https://github.com/awslabs/djl/blob/master/examples/src/test/resources/airplane1.png) and follow the inference tutorials in the [Jupyter module](https://github.com/awslabs/djl/tree/master/jupyter).\n",
    "\n",
    "2. Follow the complete example with multi-GPU support, a validation dataset, and the fit API in the [examples module](https://github.com/awslabs/djl/tree/master/examples/docs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "[2] [Gluon CV model zoo](https://gluon-cv.mxnet.io/model_zoo/classification.html) for pre-trained ResNet50 models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "12.0.2+10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
